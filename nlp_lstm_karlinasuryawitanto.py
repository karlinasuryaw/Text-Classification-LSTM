# -*- coding: utf-8 -*-
"""NLP-LSTM_KarlinaSuryaWitanto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kzVgN1lPcrwDzft2sG95Gd3pQMKWzjXY

# **Proyek I : Membuat Model NLP dengan TensorFlow - Dicoding x Kampus Merdeka**

# **DATA DIRI**

* Nama      : Karlina Surya Witanto
* ID        : M014V6051
* PT        : Universitas Udayana
* Email     : gabriella.linatan@gmail.com
* Email SIB : m014v6051@dicoding.org

# **1. Import Library & Import Dataset**

Sumber Dataset : 
*   https://www.kaggle.com/kishanyadav/inshort-news?select=inshort_news_data-2.csv
*   https://www.kaggle.com/kishanyadav/inshort-news?select=inshort_news_data-3.csv

Di bawah ini merupakan perintah untuk import dataset dari Google Drive
"""

from google.colab import drive
drive.mount('/content/drive/')

"""Import Library yang dibutuhkan"""

# Import Library yang akan digunakan

import pandas as pd
import nltk, os, re, string
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, Embedding, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
nltk.download('stopwords')

"""Import dataset yang akan digunakan. Dataset ini memiliki 3 kolom nama yaitu, news_headline, news_article, and news_category. Dataset ini juga mengandung 7 kategori berita yaitu technology,sports,politics,entertainment,world,automobile, dan science."""

dataframe = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset/inshort_news_data-2.csv')
dataframe.head(11)

dataframe.news_category.value_counts()

dataframe = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset/inshort_news_data-3.csv')
dataframe.head(11)

dataframe.news_category.value_counts()

"""Melalui hasil count data diatas, jumlah data antar kategorinya berbeda, perbedaan data ini akan mempengaruhi hasil klasifikasi, karena datanya tidak balance. Oleh karena itu, saya melakukan drop data secara manual dan menjadikan jumlah data antar kategori bernilai sama. Jumlah data antar kategori yaitu 430 berita. Berikut data yang sudah saya perbaharui. Sehingga total jumlah data yaitu 3.010 data."""

dataframe = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset/databarunlp.csv')
dataframe.head(11)

#menghitung jumlah data yang terbaru

dataframe.news_category.value_counts()

"""Di bawah ini merupakan perintah untuk menghapus kolom yang tidak digunakan."""

dataframe = dataframe.drop(columns = ['Unnamed: 0'])
dataframe.head(11)

# Total data

dataframe.shape

"""Total data yang disajikan yaitu 3.010 data yang terdiri dari 3 kolom"""

plt.figure(figsize = (12, 6))
sns.countplot(dataframe.news_category)

"""Dapat kita lihat bahwa persebaran data memiliki nilai yang sama antar kategori berita, yaitu 430 data per kategori

# **2. Data Pre-Processing**

Tahap pre-processing atau praproses data merupakan proses untuk mempersiapkan data mentah sebelum dilakukan proses lain. Pada umumnya, praproses data dilakukan dengan cara mengeliminasi data yang tidak sesuai atau mengubah data menjadi bentuk yang lebih mudah diproses oleh sistem. Praproses sangat penting dalam klasifikasi data, terutama untuk media sosial yang sebagian besar berisi kata-kata atau kalimat yang tidak formal dan tidak terstruktur serta memiliki noise yang besar. Berikut merupakan tahap-tahap pre-processing data yang saya lakukan :

**Lowercasing**
"""

dataframe.news_headline = dataframe.news_headline.apply(lambda x: x.lower())
dataframe.news_article = dataframe.news_article.apply(lambda x: x.lower())

"""**Data Cleansing**"""

#menghapus tanda baca
def cleaner(data):
    return(data.translate(str.maketrans('','', string.punctuation)))
    dataframe.news_headline = dataframe.news_headline.apply(lambda x: cleaner(x))
    dataframe.news_article = dataframe.news_article.apply(lambda x: lem(x))

#menghapus angka
def rem_numbers(data):
    return re.sub('[0-9]+','',data)
    dataframe['news_headline'].apply(rem_numbers)
    dataframe['news_article'].apply(rem_numbers)

"""**Lemmatization**"""

lemmatizer = WordNetLemmatizer()

def lem(data):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(data.split())]))
    dataframe.news_headline = dataframe.news_headline.apply(lambda x: lem(x))
    dataframe.news_article = dataframe.news_article.apply(lambda x: lem(x))

"""**Stopword Removal**"""

stwrds = stopwords.words('english')
def stopword(data):
    return(' '.join([w for w in data.split() if w not in stwrds ]))
    dataframe.news_headline = dataframe.news_headline.apply(lambda x: stopword(x))
    dataframe.news_article = dataframe.news_article.apply(lambda x: lem(x))

dataframe.head(11)

"""Proses selanjutnya yaitu proses One-Hot Encoding. One-Hot Encoding adalah teknik yang merubah setiap nilai di dalam kolom menjadi kolom baru dan mengisinya dengan nilai biner yaitu 0 dan 1. Pada case ini saya menggunakan library dari pandas untuk melakukan proses ini."""

category = pd.get_dummies(dataframe['news_category'])
dataframe = pd.concat([dataframe, category], axis=1)
dataframe = dataframe.drop('news_category', axis=1)
dataframe.head()

"""Selanjutnya saya mengubah dataframe tersebut ke dalam Numpy Array, hal ini dilakukan agar data dapat diproses saat membangun model LSTM."""

X = dataframe['news_headline'].values + '' + dataframe['news_article'].values
y = dataframe[['automobile', 'entertainment', 'politics', 'science', 'sports', 'technology', 'world']].values

y

"""Proses selanjutnya yaitu melakukan split data, yaitu 20% untuk validation dataset."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(y_train.shape)
print(y_test.shape)

"""Dapat dilihat dari hasil diatas, validation dataset sudah 20% dari total data, yaitu 602 data dengan 7 kategori berita.

**Tokenizing**

Tokenizing adalah operasi memisahkan teks menjadi potongan-potongan berupa token, bisa berupa potongan huruf, kata, atau kalimat, sebelum dianalisis lebih lanjut.
"""

tokenizer = Tokenizer(num_words=10000, oov_token='n')

tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)

sequence_train = tokenizer.texts_to_sequences(X_train)
sequence_test = tokenizer.texts_to_sequences(X_test)
padded_train = pad_sequences(sequence_train)
padded_test = pad_sequences(sequence_test)

"""# **3. Fungsi Callbacks**

Inisialisasi Fungsi Callbacks yang digunakan untuk menghentikan training data ketika akurasi mencapai diatas 97% dan validation set sudah mencapai diatas 90%.
"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.97 and logs.get('val_accuracy')>0.90):
      self.model.stop_training = True
      print("\nAkurasi training set sudah mencapai > 97% dan validation set sudah mencapai > 90%, hentikan proses training!")
callbacks = myCallback()

"""# **4. Model LSTM**

LSTM merupakan salah satu jenis dari Recurrent Neural Network (RNN) dimana dilakukan modifikasi pada RNN dengan menambahkan memory cell yang dapat menyimpan informasi untuk jangka waktu yang lama. LSTM diusulkan sebagai solusi untuk mengatasi terjadinya vanishing gradient pada RNN saat memproses data sequential yang panjang.

Membuat model sequential dan melakukan compile model dengan Adam optimizer dan loss function categorical-crossentropy (hasil klasifikasi lebih dari dua klasifikasi)
"""

model = tf.keras.Sequential([
          Embedding(input_dim=10000, output_dim=128),
          LSTM(128),
          Flatten(),
          Dropout(0.5),
          Dense(128, activation='relu'),
          Dense(64, activation='relu'),
          Dense(7, activation='softmax')
])

model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

model.summary()

"""Eksekusi LSTM Model"""

import math

batch_size=64
train_size=2408
validation_size=602

#menghitung nilai step per epoch dan val step
compute_steps_per_epoch = lambda x: int(math.ceil(1. * x / batch_size))
steps_per_epoch = compute_steps_per_epoch(train_size)
val_steps = compute_steps_per_epoch(validation_size)

nlp_model = model.fit(
    padded_train,
    y_train,
    epochs=15,
    steps_per_epoch=steps_per_epoch,
    validation_steps=val_steps,
    validation_data=(padded_test, y_test),
    verbose=1, 
    callbacks=[callbacks])

"""Dapat dilihat melalui proses diatas, model berhasil mencapai akurasi diatas 97% yaitu pada titik 97,18% dan validation set juga sudah mencapai diatas 90% yaitu pada titik 90,70% sehingga fungsi callbacks menghentikan training secara otomatis. Proses training ini memakan waktu selama 1 menit 22 detik.

# **5. Visualisasi Akurasi Data**

Menampilkan visualisasi data berdasarkan model yang sudah dibuat

**Plot Akurasi**
"""

plt.figure(figsize=(8,5))
plt.plot(nlp_model.history['accuracy'], label='train_accuracy')
plt.plot(nlp_model.history['val_accuracy'], label='validation_accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim(ymin=0, ymax=1)
plt.show()

"""**Plot Loss**"""

plt.plot(nlp_model.history['loss'])
plt.plot(nlp_model.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

"""# **Kesimpulan**

Melalui proses evaluasi model diatas, LSTM model yang sudah saya bangun mampu memberikan akurasi 97,18% dan akurasi validation set 90,70% pada 3.010 data dengan 7 kategori berita, dengan parameter-parameter di bawah ini:

*   Unit LSTM : 128
*   Batch size : 64
*   Epoch : 15
*   Fungsi aktivasi : relu, softmax
*   Optimizer : Adam
*   Loss function : categorical-crossentropy
*   Verbose : 1
*   Dropout : 0.5

Penggunaan hyperparameter diatas saya lakukan secara repetitif sampai menemukan hyperparameter terbaik dan menghasilkan akurasi yang bagus. Selain itu pada iterasi epoch ke-9 model sudah mampu memberikan akurasi sebesar 97,18% dan akurasi validation set 90,70% sehingga proses training dihentikan oleh fungsi callbacks.
"""